import tensorflow as tf
import math
print(math.log10(10))
step = 0.
weight=1.
for i in range(1000):
    drift_factor = (step + 1.) / (float(step == 0) + step)
    drift_scale = math.log10(drift_factor) * 0.09
    weight=weight*(1+drift_scale)
    if step%20==0:
        print(step, drift_factor,drift_scale,weight)
    step = step+1.
"""
1.0
0.0 1.0 0.0 1.0
20.0 1.05 0.0019070369162944281 1.1255800768488524
40.0 1.025 0.0009651478852595759 1.1553826777896032
60.0 1.0166666666666666 0.0006460726164411038 1.1734576473651692
80.0 1.0125 0.000485552869803553 1.1865328945770857
100.0 1.01 0.000388923640437832 1.1968090925712458
120.0 1.0083333333333333 0.00032437118419427165 1.2052890762779276
140.0 1.0071428571428571 0.0002781969279427678 1.21251604535674
160.0 1.00625 0.00024353040383324703 1.2188179710086329
180.0 1.0055555555555555 0.00021654627892905886 1.2244083302137712
200.0 1.005 0.0001949455580856867 1.2294339806405923
220.0 1.0045454545454546 0.00017726335766140485 1.2340003396294088
240.0 1.0041666666666667 0.00016252207769361192 1.2381856753427085
260.0 1.0038461538461538 0.00015004434307166925 1.2420497164666129
280.0 1.0035714285714286 0.00013934597065745996 1.2456390942385496
300.0 1.0033333333333334 0.00013007167867628577 1.2489909210181618
320.0 1.003125 0.0001219548676469509 1.252135222859349
340.0 1.0029411764705882 0.00011479157552183327 1.2550966404450443
360.0 1.0027777777777778 0.00010842310245335884 1.2578956477499468
380.0 1.0026315789473683 0.00010272411529281902 1.260549443891189
400.0 1.0025 9.759431629979036e-05 1.2630726180973746
420.0 1.0023809523809524 9.295248939910565e-05 1.265477653778408
440.0 1.0022727272727272 8.873216834859676e-05 1.267775316308603
460.0 1.0021739130434784 8.487843372667023e-05 1.2699749553304058
480.0 1.0020833333333334 8.134550984201335e-05 1.2720847432593643
500.0 1.002 7.809493781042219e-05 1.2741118655106933
520.0 1.001923076923077 7.509416982527804e-05 1.276062673731052
540.0 1.0018518518518518 7.231547552408052e-05 1.2779428103553625
560.0 1.0017857142857143 6.973508249648887e-05 1.2797573107027451
580.0 1.0017241379310344 6.733249446540864e-05 1.2815106873081068
600.0 1.0016666666666667 6.508994571863144e-05 1.2832070000778757
620.0 1.0016129032258065 6.299196104937183e-05 1.2848499150395583
640.0 1.0015625 6.1024998143719116e-05 1.2864427538428593
660.0 1.0015151515151515 5.917715493943892e-05 1.2879885357080831
680.0 1.0014705882352941 5.743792858939524e-05 1.2894900131652651
700.0 1.0014285714285713 5.579801571616071e-05 1.290949702656474
720.0 1.0013888888888889 5.424914593445161e-05 1.2923699108635365
740.0 1.0013513513513514 5.278394235168163e-05 1.2937527574590046
760.0 1.0013157894736842 5.1395804080330685e-05 1.2951001948487106
780.0 1.0012820512820513 5.0078806813793086e-05 1.296414025371527
800.0 1.00125 4.882761830646558e-05 1.2976959163399175
820.0 1.001219512195122 4.7637426215167426e-05 1.2989474132389587
120.0 1.0083333333333333 0.00032437118419427165 1.2052890762779276
840.0 1.0011904761904762 4.6503876242746295e-05 1.300169951348232
860.0 1.0011627906976743 4.542301890783029e-05 1.3013648660077273
880.0 1.0011363636363637 4.439126356913876e-05 1.3025334017134789
900.0 1.001111111111111 4.340533857642707e-05 1.3036767201996506
920.0 1.001086956521739 4.246225661642633e-05 1.3047959076397886
940.0 1.001063829787234 4.155928448024353e-05 1.305891981080141
960.0 1.0010416666666666 4.0693916607922814e-05 1.3069658942013482
980.0 1.0010204081632652 3.986385187082492e-05 1.308018542491039


120 1.2052890762779276
840 1.300169951348232
"""